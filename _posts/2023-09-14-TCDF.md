---
layout: single
title: "[Paper Review] Causal Discovery with Attention Based Convolutional Neural Network(TCDF)"
categories: Paper_Review
use_math: true
comments: ture
toc: true
author_profile: false
---
이번 포스팅에서는 시계열 데이터의 인과그래프를 탐색하는 논문을 리서치하던 중 인용수가 많았던 Causal Discovery with Attention_Based Convolutional Neural Network를 리뷰해보고자 합니다.  
최근 업무에서 IT서비스의 성능 표인 시계열 데이터의 인과관계를 추론하여 장애가 발생하였을 때 원인이 무엇인지 탐색하는 Root Cause Analysis(RCA)에 대한 연구를 진행하고 있습니다. 이 논문의 목적이 RCA는 아니지만 시계열의 인과관계를 추론하는 데에 연구목적과 맞기 때문에 포스팅을 하게 되었습니다.

## Abstract

먼저 초록을 정리해보면 다음과 같습니다.  

* 시계열 데이터에서 인과 관계를 발견하여 인과 그래프를 학습하는 딥러닝 framework인 TCDF를 제안한다.
* CNN의 내부 파라미터를 해석하여 원인과 결과 발생 사이의 시간 지연도 발견할 수 있다.
* 금융 및 신경과학 벤치마크에 대한 실험은 시계열 데이터에서 인과 관계를 발견하는 데 있어 TCDF의 높은 성능을 보여준다.
* 광범위하게 적용 가능하기 때문에 지식 발견 및 복잡한 시스템의 인과 관계에 대한 새로운 인사이트를 얻는 데 사용할 수 있다.

Introduction은 초록과 비슷하기 때문에 생략하도록 하겠습니다.

## Problem Statement

본 논문에서의 **Temporal causal discovery**는 길이가 $T$이고, 변수의 개수가 $N$인 시계열 데이터 $X=\{ X_1,X_2,...,X_N \} \in \mathbb{R}^{N \times T}$가 주어졌을 때, $X$의 모든 $N$개의 시계열과 원인과 결과 사이의 시간 지연 사이의 인과관계를 발견하고 시간적 인과관계 그래프로 모형화하는 것으로 정의합니다.  
인과그래프 $g=(V,E)$ 에서 노드 $v_i \in V$는 $i$번째 시계열 데이터 $X_i$를 의미하고, 엣지 $e_{i,j}$는 노드 $v_i$와 노드 $v_j$의 관계를 나타냅니다. 즉, $X_i$가 $X_j$의 원인임을 나타냅니다.  
또한, $p=<v_i,...,v_j>$는 $g$의 $v_i$부터 $v_j$까지의 path를 나타내며, temporal causal graph에서 모든 엣지는 가중치 $d(e_{i,j})$가 있으며, 이는 이는 원인 $X_i$의 발생과 결과 $X_j$의 발생 사이의 시간 지연을 나타냅니다. 예시를 확인해보면 다음과 같습니다.

![fig1]({{site.url}}/images/Paper_Review/TCDF1.png){: width="700" height="700"}

인과 관계가 복잡할 경우 Causal Discovery는 다음과 같은 어려움이 있습니다.

* 모델은 직접 원인과 간접 원인을 구별해야 합니다. fig2-a와 같이 $v_i$가 $v_j$의 간접 원인이고, $p=<v_i, v_k, v_j> \in g$일 때 Pairwise방법은 즉, 두 변수 사이의 인과관계만을 찾는 방법은 종종 이러한 구분을 할 수 없습니다. 하지만, 다변량 방법은 직접적인 인과 관계와 간접적인 인과 관계를 구별하기 위해 모든 변수를 고려합니다.
* 모델은 원인과 결과 사이의 지연이 0인 즉각적 인과 효과를 학습해야 합니다. 순간적인 영향을 무시하면 오해의 소지가 있는 해석을 초래할 수 있습니다. 이는 대부분 원인과 결과의 시간적 척도가 너무 coarse하여 인과적 정렬을 할 수 없을 때 발생합니다.
* **Confounder**의 존재는 인과관계에서 잘 알려진 challenge입니다.(fig2-b) confounder는 상관관계는 있지만 인과관계는 없기 때문에 confounder의 효과로 인해 긴과관계를 잘못 포함하지 않도록 주의해야 합니다. confounder에 대한 자세한 설명은 제가 포스팅한 [인과그래프](https://youngdong2.github.io/causal_inference/causal_inference3/)에서 확인해보실 수 있습니다.
* confounder가 관찰되지 않을 때(숨겨진(또는 잠재된) confounder) 특별한 도전이 발생합니다. 비록 얼마나 많은 숨겨진 confounder가 존재하는지조차 알 수 없을 수 있지만, 인과 추론 방법이 숨겨진 confounder의 존재를 가설로 설정하여 그 효과 사이의 잘못된 인과 관계를 학습하는 것을 방지하는 것이 중요합니다.

![fig2]({{site.url}}/images/Paper_Review/TCDF2.png){: width="700" height="700"}

## TCDF - Temporal Causal Discovery Framework

![fig3]({{site.url}}/images/Paper_Review/TCDF3.png){: width="700" height="700"}

이제 자세하게 본 논문에서 제안하는 framework에 대해 알아보겠습니다.  
TCDF는 크게 네가지 단계(Time Series Prediction, Attention Interpretation, Causal Validation, Delay Discovery)로 구성됩니다.  
TCDF는 $N$개의 독립적인 attention-based CNN으로 구성되어 있는데 모두 같은 구조로 되어있지만 fig4에서 확인할 수 있듯이 target 변수가 다릅니다. 이는 $j$번째 네트워크 $\mathcal{N_j}$는 target인 $X_j$를 예측하는 것을 의미합니다.  
$\mathcal{N_j}$에서 $X_j$를 예측하기 위해 학습될 때, attention score $a_j$는 $X_j$를 예측할 때 어떤 변수를 참조하는지 나타냅니다. $\mathcal{N_j}$에서 예측을 위해 attention된 시계열을 사용하기 때문에 이 시계열은 예측에 유용한 정보를 포함해야 하며, 이는 이 시계열이 잠재적으로 target 시계열과 인과적으로 연관되어 있음을 의미합니다. 이 네트워크의 입력데이터로 target 시계열도 포함되기 때문에 모델은 self-causation도 포착할 수 있다고 합니다.  
본 논문에서는 이러한 attention 기반 CNN을 위한 특정 아키텍쳐를 설계하여 잠재적 원인을 발견할 수 있도록 했고, 이를 AD-DSTCN이라고 정의했습니다.  
이제 네가지 단계에 대해 좀 더 구체적으로 알아보겠습니다.

### Time Series Prediction

논문에서는 TCN 아키텍쳐를 사용하여 예측모델링을 진행합니다. TCN은 1D kernel인 CNN으로 구성되어 있습니다. TCN은 입력 시계열의 과거 및 현재 값, 즉 시간 단계 1부터 시간 단계 t까지의 값을 기반으로 목표 시계열의 시간 단계 t를 예측합니다. 입력 시계열의 현재 값을 포함하면 순간 효과를 탐지할 수 있습니다. TCN은 미래에서 과거로 '누출'되는 정보가 없는 소위 causal convolution을 수행합니다.  
TCN은 입력 값이 $[X_1^1, X_1^2, ...,X_1^t,...,X_1^T]$인 시계열 $X_1$위에 kernel을 슬라이드하면서 target인 $X_2$의 각 시간을 예측합니다. 예를 들어 $X_2$의 t시점, 즉, $X_2^t$를 예측하기 위해서는 사용자가 지정한 크기가 K인 1D-kernel은 학습된 kernel 가중치 $W$와 현재 시점부터 K-1개의 이전값 사이의 dot product를 계산합니다. 즉, $W \odot [X_1^{t-K+1}, X_1^{t-K+2}, ...,X_1^{t-1},X_1^{t}]$입니다. 하지만 이러면 초기의 값들을 예측할 때는 데이터가 부족해지기 때문에 없는 데이터를 zero padding을 통해 채워줍니다.  
원래 TCN에서서는 활성함수로 ReLU를 사용하지만, 본 논문에서는 PReLU를 사용합니다. 그 이유로 기존 ReLU에 비해 계산 비용이 거의 들지 않고, 과적합 위험이 거의 없어 model fitting을 개선하기 때문이라고 설명합니다.

### Dilations

하나의 layer만 있는 TCN의 경우, receptive field는 사용자가 지정한 커널 크기 K와 같습니다. 인과 관계를 성공적으로 발견하려면 receptive field가 적어도 원인과 결과 사이의 delay만큼 커야합니다. receptive field를 늘리려면 커널 크기를 늘리거나 hidden layer의 수를 추가해야하는 데, 1D 커널을 사용하는 CNN은 레이어 수에 따라 선형적으로 증가하는 receptive field를 가지므로 큰 receptive field가 필요할 때 계산 비용이 많이 듭니다.  
receptive field를 크게 하면서 연산량을 적게 하기 위해 dilated convolutions를 도입합니다. dilated convolutions는 특정 스텝 크기 $f$로 입력 값을 건너뛰는 방식으로 커널크기보다 큰 영역에 적용할 수 있습니다. dilation factor인 $f$는 dilation coefficient $c$에 따라 기하급수적으로 증가하게 됩니다. 즉, $f=c^l$입니다. ($l$은 layer 개수) 아래는 예시입니다.

![fig4]({{site.url}}/images/Paper_Review/TCDF4.png){: width="700" height="700"}

$f$가 기하급수적으로 증가하면 dilated convolution이 적용된 네트워크는 해상도나 coverage의 손실 없이 더 coarse한 규모로 작동할 수 있습니다. receptive field R은 다음과 같습니다.  

$\begin{align} R_{D-TCN}=1+\sum_{l=0}^L (K-1) \cdot c^l \end{align}$
