---
layout: single
title: "[Paper Review] You Only Look Once(YOLO)"
categories: AI/ML
use_math: true
comments: ture
toc: true
author_profile: false
---
이번 포스팅에서는 지난 7월에 풀입스쿨 논문요약 스터디에서 다뤘던 YOLO에 대해 요약해보려 합니다.  
제조 AI에 관심이 있는데 CV쪽을 알아두면 좋을 것 같아 참여하게 되었습니다.

## Abstract

### Object Detection의 새로운 접근법

* 선행 연구에서는 분류기를 사용하여 탐지를 수행.
* 본 논문에서는 bounding box와 관련 클래스의 확률에 대한 회귀문제로 해결.
* 전체 파이프라인이 하나의 네트워크로 이루어져 있기 때문에 end-to-end로 최적화가 가능.
* YOLO의 아키텍쳐는 초당 45프레임의 이미지를 실시간으로 처리할 수 있기 때문에 굉장히 빠르다.

## Introduction

![fig1]({{site.url}}/images/Paper_Review/YOLO1.png){: width="700" height="700"}

* YOLO는 객체 탐지를 이미지 픽셀에서 bounding box의 좌표와 각 클래스의 확률을 구하는 문제로 해결한다.
* End-to-end 방색의 통합된 구조로 되어 있으며, 이미지를 CNN에 한 번 평가하는 것을 통해서 동시에 다수의 bounding box와 클래스 확률을 구하게 된다.
* 이러한 통합모델로 인해 YOLO는 몇 가지 장점을 가지게 됨  
  1. Titan X에서 45fps를 달성하며 빠른 버전의 경우 150fps의 빠른 성능을 보이고 다른 실시간 시스템 대비 2배 이상의 mAP의 성능을 보인다.
  2. Sliding window 방식이 아닌 CNN을 사용하는 것으로 인해 전체 이미지를 보게끔 유도되어 각 class에 대한 표현을 더 잘 학습함.
  3. 일반화된 Object의 표현을 학습한다. 실험적으로 자연의 dataset을 학습시킨 후 학습시킨 네트워크에 artwork 이미지를 입력했을 때, 선행 연구 대비 좋은 detection 성능을 보인다.

## Unified Detection

* YOLO는 단일 CNN 모델 하나로 feature 추출, bounding box 계산, 클래스 분류를 모두 수행한다. 즉, 원본 이미지를 입력으로 받은 모델이 객체 탐지에 필요한 모든 연산을 수행할 수 있다는 의미다.
* YOLO는 이미지 전체를 보고 Bounding box를 예측할 수 있다. 즉, 모델이 이미지의 전역적인 특징을 잘 이용할 수 있다는 의미.
* Unified Detection답게 bounding box regression과 multi-class classification을 동시에 추론할 수 있다.
* 위와 같은 이점들로 인해 YOLO는 높은 mAP를 유지하면서 End-to-End 학습이 가능하고, 실시간의 추론속도가 가능하다.

![fig2]({{site.url}}/images/Paper_Review/YOLO2.png){: width="700" height="700"}

* 위 그림과 같이 입력 이미지를 S x S 그리드로 나눈다. 만약 어떤 객체의 중점이 특정 그리드 셀 안에 있다면, 그리드 셀이 그 객체를 검출해야 한다.
* 각각의 그리드 셀은 B개의 bounding box와 bounding box에 대한 confidence score를 예측한다. Confidence score란 bounding box가 객체를 포함한다는 것을 얼마다 믿을만한지, 또 예측한 bounding box가 얼마나 정확한지를 나타낸다.
* 각각의 bounding box는 5개의 예측값으로 구성된다.(x, y, w, h, confidence)
* (x, y) 좌표 쌍은 bounding box 중심의 그리드 셀 내 상대 위치를 뜻한다. 절대위치가 아니라 그리드 셀 내의 상대위치기 때문에 0~1 사이의 값을 가진다.
* (w, h) 쌍은 bounding box의 상대 너비와 상대 높이를 뜻한다. 이 때 (w, h)는 이미지 전체의 너비와 높이를 1이라고 했을 때 bounding box의 너비와 높이가 몇인지를 상대적인 값으로 나타낸다.
* Confidence는 confidence score로 $Pr(Object)*IOU_{pred}^{truth}$이고 그리드 셀에 객체가 없다면 $Pr(Object)=0$, 그리드 셀에 어떤 객체가 확실히 있다고 예측했다면 $Pr(Object)=1$이다.
* 각 그리드 셀은 conditional class probabilities를 예측한다.  
$C(Conditional \, class  \, prob)=Pr(Class_i|Object)$
* 이는 그리드 셀 안에 객체가 있다는 조건 하에 그 객체가 어떤 클래스인지에 대한 조건부확률을 의미한다.
* 그리드 셀에 몇 개 의 bounding box가 있는지 상관없이 하나의 그리드 셋에는 오직 하나의 클래스에 대한 확률 값만 구한다.
* Test time에는 conditional class prob와 개별 bounding box의 confidence score를 곱해주는데, 이를 각 bounding box에 대한 class-specific confidence score라고 부르고 다음과 같이 나타낸다.  
$Pr(Class_i|Object)*Pr(Object)*IOU_{pred}^{truth}=Pr(Class_i)*IOU_{pred}^{truth}$
* 이 score는 bounding box에 특정 클래스 객체가 나타날 확률과 예측된 bounding box가 그 클래스 객체에 얼마나 잘 들어맞는지를 나타낸다.

## Network Design

![fig3]({{site.url}}/images/Paper_Review/YOLO3.png){: width="700" height="700"}

* YOLO는 CNN구조로 되어있다.
* 앞단은 convolutional layer이고, 이어 fc layer로 구성된다.
* Convolutional layer는 이미지에서 특징을 추출하고, fc layer는 클래스 확률과 bounding box의 좌표를 추론한다.
* YOLO는 24개의 conv layer와 2개의 fc layer로 구성된다.

## Training

* 먼저 1,000개의 클래스를 갖는 ImageNet 데이터 셋으로 YOLO의 컨볼루션 계층을 pre-train 시켰다. 24개의 conv layer 중 앞단의 20개의 conv layer를 pretrain한다.
* ImageNet은 분류를 위한 데이터 셋이다. 따라서 pre-train된 분류 모델을 object detection모델로 바꾸어야 한다. Pretrain된 20개의 conv layer 뒤에 4개의 conv layer 및 2개의 fc계층을 추가했다.
* 최종 아웃풋은 클래스 확률과 bounding box 위치정보.
* YOLO의 마지막 계층에는 linear activation function을 적용했고, 나머지 모든 계층에는 leaky ReLU를 적용.
* Loss는 SSE를 기반으로 하는데 그 이유는 최적화가 쉽기 때문. 하지만 이를 최적화하는 것이 YOLO의 최종 목적인 mAP를 높이는 것과 일치하지는 않는다.
* 또 다른 문제도 있다. YOLO는 S x S개의 그리드 셀을 예측하는데 거의 모든 이미지에서 대다수의 그리드 셋에는 객체가 존재하지 않는다.
* 이런 불균형은 YOLO가 모든 그리드 셀에서 confidence=0이라고 예측하게 학습될 수 있다.
* YOLO에서는 이를 해결하기 위해 객체가 존재하는 bounding box의 confidence loss 가중치를 늘리고 존재하지 않는 bounding box의 confidence loss 가중치는 줄인다.
* SSE는 또 다른 문제도 있다. SSE는 큰 bounding box와 작은 bounding box에 대해 모두 동일한 가중치로 loss를 계산한다. 하지만 작은 box가 큰 box보다 작은 위치 변화에 민감하다. 이를 개선하기 위해 bounding box의 너비와 높이에 square root를 취했다.
* YOLO는 하나의 그리드 셀 당 여러 개의 bounding box를 예측한다.
* 학습 시 하나의 bounding box predictor가 하나의 객체에 대한 책임이 있어야 한다.
* 따라서 여러 개의 bounding box 중 하나를 선택해야 한다. 이를 위해 예측된 여러 bounding box 중 실제 객체를 감싸는 ground-truth bounding box와의 IOU가 가장 큰 것을 선택한다.
* 이렇게 훈련된 bounding box predictor는 특정 크기, aspect ratios, 객체의 클래스를 잘 예측하게 된다.

## Loss Function

![fig4]({{site.url}}/images/Paper_Review/YOLO4.png){: width="700" height="700"}

* 여기서 $1_{ij}^{obj}$는 $i$번째 그리드 셀에 있는 $j$번째 바운딩박스에 객체가 존재하는지를 나타낸다.
위 수식을 차례대로 설명해보면,

  1. 객체가 존재하는 그리드 셀 $i$의 bounding box predictor $j$에 대해, x와 y의 loss를 게산한다.
  2. 객체가 존재하는 그리드 셀 $i$의 bounding box predictor $j$에 대해, w와 h의 loss를 게산한다. box크기에 대한 민감도를 줄이기 위해 제곱근을 취한 후 SSE를 구한다.
  3. 객체에 존재하는 그리드 셀 $i$의 bounding box predictor $j$에 대해, confidence score의 loss를 계산한다.
  4. 객체에 존재하지 않는 그리드 셀 $i$의 bounding box predictor $j$에 대해, confidence score의 loss를 계산한다.
  5. 객체가 존재하는 그리드 셀 $i$에 대해, conditional class prob의 loss를 계산한다.
